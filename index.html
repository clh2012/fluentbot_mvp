<div style="margin-top: 1em;">
  <button id="recordBtn">ðŸŽ¤ Start Recording</button>
  <p id="recordingStatus"></p>
  <audio id="audioPlayback" controls style="display:none; margin-top:1em;"></audio>
  <progress id="audioLevel" value="0" max="1" style="width: 100%; display: none; margin-top: 0.5em;"></progress>

  <!-- Transcript text box -->
  <textarea id="transcriptBox" rows="6" style="width:100%; margin-top: 1em;" placeholder="Transcript will appear here..." readonly></textarea>

  <!-- Dialogue container -->
  <div id="dialogue" style="margin-top: 1em; border: 1px solid #ccc; padding: 1em; max-height: 300px; overflow-y: auto;">
    <h3>Conversation</h3>
    <!-- Each message will be appended here -->
  </div>

  <!-- 11 Labs playback button -->
  <button id="playVoiceBtn" style="margin-top: 1em; display:none;">ðŸ”Š Play Voice</button>
</div>

<script>
  let mediaRecorder;
  let audioChunks = [];
  let audioContext;
  let analyser;
  let dataArray;
  let animationId;

  const recordBtn = document.getElementById("recordBtn");
  const status = document.getElementById("recordingStatus");
  const audioPlayback = document.getElementById("audioPlayback");
  const audioLevel = document.getElementById("audioLevel");
  const transcriptBox = document.getElementById("transcriptBox");
  const dialogue = document.getElementById("dialogue");
  const playVoiceBtn = document.getElementById("playVoiceBtn");

  let lastTranscript = "";

  recordBtn.addEventListener("click", async () => {
    if (!mediaRecorder || mediaRecorder.state === "inactive") {
      // Start recording
      const stream = await navigator.mediaDevices.getUserMedia({ audio: true });

      audioContext = new AudioContext();
      const source = audioContext.createMediaStreamSource(stream);
      analyser = audioContext.createAnalyser();
      source.connect(analyser);
      analyser.fftSize = 256;
      const bufferLength = analyser.frequencyBinCount;
      dataArray = new Uint8Array(bufferLength);

      audioLevel.style.display = "block";

      function updateMeter() {
        analyser.getByteFrequencyData(dataArray);
        let sum = 0;
        for (let i = 0; i < bufferLength; i++) {
          sum += dataArray[i];
        }
        const avg = sum / bufferLength / 255;
        audioLevel.value = avg;
        animationId = requestAnimationFrame(updateMeter);
      }
      updateMeter();

      mediaRecorder = new MediaRecorder(stream);
      audioChunks = [];

      mediaRecorder.ondataavailable = (event) => {
        if (event.data.size > 0) audioChunks.push(event.data);
      };

      mediaRecorder.onstop = async () => {
        cancelAnimationFrame(animationId);
        audioLevel.style.display = "none";
        audioContext.close();

        const blob = new Blob(audioChunks, { type: "audio/webm" });
        const audioURL = URL.createObjectURL(blob);
        audioPlayback.src = audioURL;
        audioPlayback.style.display = "block";
        status.textContent = "Recording complete.";

        // Send the audio blob to your backend to get transcript
        try {
          status.textContent = "Transcribing...";
          const formData = new FormData();
          formData.append("audio", blob, "recording.webm");

          const response = await fetch("/api/transcribe", {
            method: "POST",
            body: formData,
          });

          if (!response.ok) throw new Error("Transcription failed");

          const data = await response.json();
          transcriptBox.value = data.transcript || "[No transcript returned]";
          status.textContent = "Transcription complete.";

          // Append transcript to dialogue
          appendDialogue("User", data.transcript);

          // Enable the play voice button for 11 Labs TTS
          playVoiceBtn.style.display = "inline-block";
          lastTranscript = data.transcript;
        } catch (err) {
          console.error(err);
          status.textContent = "Error during transcription.";
        }
      };

      mediaRecorder.start();
      recordBtn.textContent = "ðŸ›‘ Stop Recording";
      status.textContent = "Recording...";
    } else if (mediaRecorder.state === "recording") {
      mediaRecorder.stop();
      recordBtn.textContent = "ðŸŽ¤ Start Recording";
      status.textContent = "Processing audio...";
    }
  });

  // Append message to dialogue container
  function appendDialogue(speaker, text) {
    if (!text) return;
    const message = document.createElement("p");
    message.innerHTML = `<strong>${speaker}:</strong> ${text}`;
    dialogue.appendChild(message);
    dialogue.scrollTop = dialogue.scrollHeight; // auto scroll down
  }

  // 11 Labs TTS voice playback example
  playVoiceBtn.addEventListener("click", async () => {
    if (!lastTranscript) return;

    status.textContent = "Generating voice...";
    try {
      // Call your 11 Labs TTS backend endpoint here, e.g., /api/tts
      const response = await fetch("/api/tts", {
        method: "POST",
        headers: { "Content-Type": "application/json" },
        body: JSON.stringify({ text: lastTranscript }),
      });

      if (!response.ok) throw new Error("TTS generation failed");

      const data = await response.json();
      // data.audioUrl should be URL of generated voice audio

      audioPlayback.src = data.audioUrl;
      audioPlayback.style.display = "block";
      audioPlayback.play();

      appendDialogue("Bot", lastTranscript);
      status.textContent = "Voice playback ready.";
    } catch (err) {
      console.error(err);
      status.textContent = "Error generating voice.";
    }
  });
</script>
